# Strategy & Metrics (SM) - Software Domain
## HAIAMM Assessment Questionnaire v2.0

**Practice:** Strategy & Metrics (SM)
**Domain:** Software
**Purpose:** Assess organizational maturity in strategic planning and metrics for Human Assisted Intelligence in software security

---

## Instructions

- Answer each question honestly based on **current, implemented practices** (not plans or aspirations)
- **"Yes" requires evidence** - Document proof for each affirmative answer
- **Answer progressively** - Complete all Level 1 questions before Level 2
- **Level progression** - Achieve ALL questions at lower level before advancing
- **Partial implementation = "No"** - Practice must be complete and systematic

---

## Level 1: Foundational
**Objective:** Establish unified roadmap for software security operated by AI

### Question 1: Risk Assessment and HAI Inventory

**Q1.1:** Have you created an inventory of Human Assisted Intelligence systems that perform software security functions (e.g., SAST/DAST tools, AI code review, automated security testing, dependency scanning)?

**Evidence Required:**
- [ ] Documented inventory listing all HAI systems used for software security
- [ ] Risk assessment identifying potential failure scenarios for each HAI system
- [ ] Business impact analysis (what happens if HAI misses vulnerabilities or generates false positives)
- [ ] Document dated within last 12 months

**Answer:** ☐ Yes  ☐ No

**Evidence Location:** _________________________________

**Notes:** ___________________________________________

---

### Question 2: Strategic Roadmap and Governance

**Q1.2:** Do you have a documented strategic roadmap for governing Human Assisted Intelligence in software security, including executive sponsorship, governance structure, and basic effectiveness metrics?

**Evidence Required:**
- [ ] 12-18 month strategic roadmap document
- [ ] Identified executive sponsor (CISO, Head of AppSec, VP Engineering)
- [ ] Defined governance structure (who oversees HAI software security decisions)
- [ ] Basic metrics established (e.g., # of applications scanned, # of AI findings, accuracy rate)
- [ ] Roadmap approved by leadership and communicated to teams

**Answer:** ☐ Yes  ☐ No

**Evidence Location:** _________________________________

**Notes:** ___________________________________________

---

### Question 3: Foundational Threat Intelligence Integration

**Q1.3:** Have you integrated foundational threat intelligence sources (such as CISA KEV, NVD, GitHub Security Advisories) into your HAI software security tools to improve vulnerability prioritization?

**Evidence Required:**
- [ ] Threat intelligence sources identified and documented
- [ ] Integration with at least 80% of HAI security tools (SAST, DAST, SCA)
- [ ] Evidence of enrichment (AI findings include threat intelligence context)
- [ ] Metrics showing threat intelligence improves prioritization (e.g., exploited vulnerabilities prioritized higher)
- [ ] High/critical findings enriched within 1 hour

**Answer:** ☐ Yes  ☐ No

**Evidence Location:** _________________________________

**Notes:** ___________________________________________

---

**Level 1 Score:** _____ / 3 questions answered "Yes"

**Level 1 Achieved:** ☐ Yes (3/3) ☐ No (< 3/3)

---

## Level 2: Comprehensive
**Objective:** Classify applications and measure AI agent effectiveness by software risk tier

**Prerequisites:** ALL Level 1 questions must be "Yes" to proceed to Level 2

### Question 4: Application Classification and Risk-Based Oversight

**Q2.1:** Have you classified your applications into risk tiers based on business criticality and data sensitivity, with differentiated levels of HAI autonomy and human oversight for each tier?

**Evidence Required:**
- [ ] Application classification framework documented (e.g., Critical, High, Medium, Low)
- [ ] Classification considers: business criticality, data sensitivity, regulatory scope, attack surface
- [ ] Different HAI oversight levels defined for each tier (e.g., Critical = human validates all findings, Low = AI operates autonomously)
- [ ] All applications classified according to framework
- [ ] Classification reviewed and updated at least annually

**Answer:** ☐ Yes  ☐ No

**Evidence Location:** _________________________________

**Notes:** ___________________________________________

---

### Question 5: Per-Classification Effectiveness Metrics

**Q2.2:** Do you measure HAI software security effectiveness separately for each application risk tier, tracking metrics such as true positive rate, false positive rate, coverage completeness, and developer adoption?

**Evidence Required:**
- [ ] Metrics defined for each application classification tier
- [ ] Tracking true positive rate (% of AI findings that are real vulnerabilities)
- [ ] Tracking false positive rate (% of AI findings that are incorrect)
- [ ] Tracking coverage completeness (% of code analyzed by AI)
- [ ] Goals established per tier (e.g., Critical apps: >90% true positive, <10% false positive)
- [ ] Metrics reviewed at least quarterly

**Answer:** ☐ Yes  ☐ No

**Evidence Location:** _________________________________

**Notes:** ___________________________________________

---

### Question 6: Threat Intelligence Classification and ROI

**Q2.3:** Have you classified software threats by organizational relevance (based on your technology stack) and demonstrated measurable ROI from threat intelligence integration?

**Evidence Required:**
- [ ] Threat classification framework (Critical, High, Medium, Low relevance)
- [ ] Classification based on organization's actual technology usage (languages, frameworks, dependencies)
- [ ] Different AI response actions for each threat class (e.g., Critical = immediate automated remediation, Low = informational)
- [ ] Evidence of cross-domain threat correlation
- [ ] ROI metrics documented (e.g., false positive reduction >30%, vulnerability remediation >50% faster)

**Answer:** ☐ Yes  ☐ No

**Evidence Location:** _________________________________

**Notes:** ___________________________________________

---

**Level 2 Score:** _____ / 3 questions answered "Yes"

**Level 2 Achieved:** ☐ Yes (3/3) ☐ No (< 3/3)

---

## Level 3: Industry-Leading
**Objective:** Align AI software security investment with demonstrable vulnerability reduction and secure development efficiency

**Prerequisites:** ALL Level 2 questions must be "Yes" to proceed to Level 3

### Question 7: Industry Benchmarking and Cost Comparison

**Q3.1:** Do you conduct periodic (at least annually) industry-wide cost comparisons for HAI software security, benchmarking your investment and effectiveness against industry peers and vulnerability data?

**Evidence Required:**
- [ ] Annual benchmarking analysis completed
- [ ] Comparison metrics documented (cost per application, vulnerability density reduction, incident prevention)
- [ ] External data sources used (industry reports, vulnerability databases, peer benchmarking)
- [ ] Results shared with executive leadership
- [ ] Benchmarking drives investment decisions and tool selection

**Answer:** ☐ Yes  ☐ No

**Evidence Location:** _________________________________

**Notes:** ___________________________________________

---

### Question 8: Historical ROI Tracking and Demonstrable Outcomes

**Q3.2:** Do you collect and analyze historical data (minimum 12 months) on HAI software security spend correlated with measurable vulnerability reduction, development efficiency improvements, and demonstrable ROI?

**Evidence Required:**
- [ ] Historical tracking (12+ months) of HAI security investment
- [ ] Vulnerability reduction metrics tracked over time (production vulnerabilities, penetration test findings)
- [ ] Development efficiency metrics (time to fix, shift-left effectiveness, developer satisfaction)
- [ ] ROI calculated with specific metrics (breach cost avoidance, remediation efficiency, AppSec productivity)
- [ ] Executive-level ROI presentation delivered to leadership/board
- [ ] Evidence of investment decisions based on ROI analysis

**Answer:** ☐ Yes  ☐ No

**Evidence Location:** _________________________________

**Notes:** ___________________________________________

---

### Question 9: Original Threat Intelligence Production and Industry Contribution

**Q3.3:** Do you produce and share original software security threat intelligence with the industry through responsible vulnerability disclosure, malicious package reporting, exploitation technique documentation, or trend analysis?

**Evidence Required:**
- [ ] At least 2 original threat intelligence contributions per year
- [ ] Contributions include: vulnerability disclosures, malicious package reports, exploitation technique docs, or trend analysis
- [ ] Public evidence (CVE assignments, security advisories, blog posts, conference presentations)
- [ ] Participation in industry ISACs or threat intelligence sharing communities
- [ ] Documented impact (e.g., vendor patches released, malicious packages removed, industry awareness improved)

**Answer:** ☐ Yes  ☐ No

**Evidence Location:** _________________________________

**Notes:** ___________________________________________

---

**Level 3 Score:** _____ / 3 questions answered "Yes"

**Level 3 Achieved:** ☐ Yes (3/3) ☐ No (< 3/3)

---

## Practice Score Calculation

### Simplified Scoring (Recommended)

```
Level 1 Achieved (all 3 "Yes"): 1.0 point
Level 2 Achieved (all 3 "Yes"): +1.0 point (total 2.0)
Level 3 Achieved (all 3 "Yes"): +1.0 point (total 3.0)
```

**SM-Software Practice Score:** _______ / 3.0

### Precise Scoring (For Formal Audits)

```
L1_score = (L1 "Yes" answers) / 3
L2_score = (L2 "Yes" answers) / 3 × L1_score
L3_score = (L3 "Yes" answers) / 3 × L2_score

Practice Score = L1_score + L2_score + L3_score
```

**SM-Software Practice Score (Precise):** _______ / 3.0

---

## Assessment Summary

**Assessment Date:** _________________________________

**Assessor(s):** _____________________________________

**HAI System(s) Assessed:** __________________________

**Overall Maturity Level:**
- ☐ Level 0 (Score < 1.0): Ad-hoc, no formal strategic planning
- ☐ Level 1 (Score 1.0 - 1.9): Foundational roadmap and metrics established
- ☐ Level 2 (Score 2.0 - 2.9): Comprehensive classification and measurement
- ☐ Level 3 (Score 3.0): Industry-leading ROI demonstration and contribution

**Strengths:**

_________________________________________________________________

**Gaps:**

_________________________________________________________________

**Priority Improvements:**

_________________________________________________________________

**Re-Assessment Date:** _________________________________

---

## Evidence Repository

Link all evidence documents here for audit trail:

| Question | Evidence Document | Location | Date | Owner |
|----------|------------------|----------|------|-------|
| Q1.1 | | | | |
| Q1.2 | | | | |
| Q1.3 | | | | |
| Q2.1 | | | | |
| Q2.2 | | | | |
| Q2.3 | | | | |
| Q3.1 | | | | |
| Q3.2 | | | | |
| Q3.3 | | | | |

---

**Document Version:** 2.0
**Last Updated:** 2025-12-29
**Next Review:** Quarterly or after significant HAI system changes
